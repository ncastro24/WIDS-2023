---
title: "Midterm Report"
author: Ellie Strande, Nathan Nguyen, Nayeli Castro
format:
    html:
        code-background: true
        toc: true
# engine: knitr
# jupyter: python3
---

#### Imports
```{python}
import warnings 
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import statsmodels.api as sm

from sklearn.model_selection import train_test_split 
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression # Linear Regression Model
from sklearn.preprocessing import StandardScaler #Z-score variables
from sklearn.metrics import mean_squared_error, r2_score #model evaluation

from sklearn.model_selection import KFold # k-fold cv
from sklearn.model_selection import LeaveOneOut #LOO cv

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor

import xgboost as xgb


%matplotlib inline
```

# Data Cleaning

```{python}

#Drop missing values 
trainData = trainData.dropna()

#Find which variables are strings/continuous (objects)
typesDF = pd.DataFrame(trainData.dtypes)
typesDF

#Create list of predictor variables
preds = list(trainData)

preds.remove('index')
preds.remove('lat')
preds.remove('startdate')
preds.remove('climateregions__climateregion')
preds.remove('mjo1d__phase')
preds.remove('mei__meirank')
preds.remove('mei__nip')
preds.remove('contest-tmp2m-14d__tmp2m')

X = trainData[preds]
y = trainData['contest-tmp2m-14d__tmp2m']

```

## Simple Linear Regression Model

```{python}

#Train test split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 5)

#Standardize/Z score
z = StandardScaler()
X_train[preds] = z.fit_transform(X_train[preds])
X_val[preds] = z.transform(X_val[preds])

#Create and fit model
lr = LinearRegression()
lr.fit(X_train, y_train)

#Predictions
y_pred = lr.predict(X_val)

#Print Training MSE
print('Train MSE: ', mean_squared_error(y_train, lr.predict(X_train)))

#Print Testing MSE
print('Test MSE: ', mean_squared_error(y_val, y_pred))
```




You may also want to use a bulleted list!

- just
- like
- this
- one

Or maybe an ordered list!

1. like
2. this
3. one

To emphasize your point you might want to use *italics* or **bold**. 

To have something appear as code (using a monospace font), surround everything that is code with ticks \`like this\`, so that it shows up `like this`.

If you're the math-y type, you will be happy to know that you can write LaTeX in .qmd files as well! For a full line formula, use two \$\$ dollar signs at the begining and end of your LaTeX:

```
$$
\frac{\partial f}{\partial y} = x + 2y
$$
```

will appear as 

For in-line LaTeX just use one \$ in your text. For example, $x + \beta$

You may also want to put a [link](https://www.google.com) to a website for reference. Or, insert an image!

![Caption](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/gradient-descent-learning-rate.png)

## Callout Blocks
One really cool thing about markdown documents like Quarto is that you can use a **callout block**. These are helpful for two reasons:

- call-out blocks highlight sections of text that interrupt the flow of your regular text. Maybe it's a definition, a warning, or a sidenote
- when rendering your document to HTML, callout blocks are interactive and can be collapsed and opened. 

::: {.callout-warning}
## My First Callout Block!
Say things here
:::

IF you're rendering to HTML (PDFs are not interactive and therefore cannot collapse callout blocks) you can use the `collapse="true"` argument in a callout to make the callout block not show unless the user clicks on it. I use this in my courses to hide material that's useful, but not necessary, like these calculus reviews:




$$
\frac{d}{d x} (f(x) + g(x)) = \frac{d}{d x}f(x) + \frac{d}{d x}g(x)
$$

$$
\frac{d}{d x} (f(x) - g(x)) = \frac{d}{d x}f(x) - \frac{d}{d x}g(x)
$$

### The Constant Rule
The derivative of a constant multiplied by a function is the constant multiplied by the derivative:

$$
\frac{d}{d x} (k * f(x)) = k * \frac{d}{d x}f(x)
$$
:::

or this question and answer:

::: {.callout-warning}
## Question
- We want to *decrease* $f$, so what should we do to $x$ and $y$?
- Which one should we decrease more? $x$ or $y$?
:::

::: {.callout-warning collapse="true"}
## Answer
We should *decrease* both $x$ and $y$, and we should decrease $y$ more becasue it has a bigger impact than $x$ at this specific point $(2,3)$.

It turns out that we'll take a step in the direction $-\begin{bmatrix} 7 \\ 8 \end{bmatrix}$!

But we'll save the details for CPSC 393...
:::

## Code Blocks
One of the most useful things about markdown documents like this is that you can intersperse your writing with code!

For example, this R code:


or this python code:

```{python}
x = 10
y = 2
print(x + y)
```

## Notes
One important thing to remember is that when you render a .qmd file, all the code runs, so if your code does something computationally expensive you may not want to run it in your .qmd file. 

One option is to use the `eval: false` argument when setting up your codeblock:

```{python}
#| eval: false
print("Don't Run Me")
```
```{python}
#| eval: true
print("Do Run Me")
```

This is useful for showing someone what code you used without needing to show them the output or comupute the code. For example you could show the way you set up your model without building or training it, and then load in a pre-trained model later to show them any output.

```{python}
#| eval: false
# This code will not run it's just
# for show
import pandas as pd
import numpy as np
from plotnine import *
from sklearn.linear_model import LinearRegression # Linear Regression Model
from sklearn.preprocessing import StandardScaler #Z-score variables
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #model evaluation
# to save model
import pickle
df = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/amazon-books.txt",
                 sep = "\t")
df = df.dropna()
# set up X and y
predictors = ["List Price", "NumPages", "Weight (oz)", "Thick", "Height", "Width"]
X = df[predictors]
y = df["Amazon Price"]
# build model
m = LinearRegression()
m.fit(X,y)
# save trained model
with open("model.pkl", "wb") as f:
    pickle.dump(m,f)
```

The above code doesn't run, it's just to show people what you did. 

The code below WILL run. It loads a pre-trained model and actually does the predictions.

